---
title: "Missing data and imputation"
subtitle: "This is a Report Template Quarto"
author: "Tharun Teja Gandham, Mourya Rai Papolu (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

## Introduction

Across healthcare, social sciences and finance sectors missing data continues as an extensive problem because it creates biased outcomes and reduces research reliability and weakens research conclusions unless appropriate data handling methods are used [@salgado2016missing]. The Electronic Health Records (EHR) dataset faces high vulnerability to missing data from technological breakdowns as well as human mistakes and data loss incidents. In various fields missing data originates from survey non-responses and equipment failures as well as data entry errors [@alwateer2024missing]. Improper handling of missing data leads to severe consequences on the quality of research findings and decision-making processes particularly when analyzing clinical epidemiology and structured datasets according to Afkanpour, Hosseinzadeh, and Tabesh (2024).

The solution for this issue requires researchers to implement imputation techniques that provide estimated values for missing data points. Various imputation methods exist which include basic methods like mean substitution and advanced techniques using multiple imputation along with machine learning models. The selection of a method for imputation requires knowledge of the missing data pattern because it can be MCAR or MAR or MNAR. Mean imputation and listwise deletion remain straightforward to use yet they produce biases or overlook missing value uncertainty [@pedersen2017missing]. Research by Alwateer et al. (2024) proves that multiple imputation and K-Nearest Neighbors (KNN) and Random Forests machine learning models effectively manage complex datasets that have high missingness levels and non-linear relationships. Proper evaluation procedures must be conducted because these techniques need to balance both computational efficiency and accuracy levels.

The selection of imputation methods relies on both the dataset characteristics and the nature of missing data according to recent research findings. Research on 58 articles demonstrates traditional methods work best for data sets with straightforward missingness patterns but deep learning and hybrid models deliver superior results for complex data patterns (Afkanpour, Hosseinzadeh, and Tabesh, 2024). The social science field demonstrates that semi-parametric techniques including predictive mean matching with hot deck imputation deliver superior results compared to fully parametric methods for item-nonresponse data according to Citeseerx (2024). The research emphasizes the requirement for imputation approaches which match the unique needs of datasets along with their associated research goals.
The analysis evaluates five imputation approaches based on mean, mode, regression, K-Nearest Neighbors (KNN), and Multiple Imputation by Chained Equations (MICE) through application on the well-known Titanic dataset. The research objective is to determine which of these five methods produces the best results for handling missing data in structured datasets so researchers can guarantee data reliability and improve their research findings' validity. The research findings from this analysis will expand existing knowledge about imputation methods and supply actionable tips to scientists and practitioners who need to handle missing data.


## Literature Review

Missing data is common problem in data science, which affects the accuracy and reliability of analyses across different domains. This section summarizes major findings from existing research on ‘missing data imputation’, which focus on old traditional and new advanced methods and their applications, limitations.

1. Mechanisms of Missing Data:

How missing data is dealt with in practice depends entirely on what type of missingness it has, so that it can be classified as Missing Completely at Random (MCAR), Missing at Random (MAR), or Not Missing at Random (NMAR). In terms of clinical research, MAR is a very common means of missing data, often being the result of careening into trees or equipment breaking [@salgado2016missing]. Identifying these mechanisms is important for correctly chosen allocation mechanisms. [@lee2024prevention] stress that robustness across time for data acquisition must be used to prevent missing data in clinical studies together with monitoring as close to real-time as possible.

2. Traditional Imputation Methods:

Traditional methods such as mean substitution, regression imputation, and listwise deletion are simple, widely used, but also introduce bias or reduce statistical power. When mean substitution is performed, the original distribution of data is disrupted as one point is substituting every missing figure. It may be biased or not exceed thoroughness in cases where people can participate for complete information and which doesn't require objectives. Sample size is reduced by listwise deletion hence leading to loss of statistical power [@alwateer2024missing]. However, traditional methods still remain popular because of their simplicity of implementation and the manner in which they focus on bias rather than noise in data. [@yadav2024computational] offer a comprehensive overview of these methods, pointing out that their main employment is low-dimensional data sets with simple missing patterns

3. Advanced Imputation Techniques:

Multiple imputation alongside machine learning and deep learning models demonstrate effectiveness when dealing with complex datasets having substantial amounts of missing data. Referred to by Little & Rubin [@yadav2024computational] Multiple Imputation by Chained Equations (MICE) produces numerous datasets with imputed values which it then combines to deal with uncertainty in clinical research. The algorithm MissForest uses Random Forests to analyze mixed-type data while showing reliable results across MCAR, MAR, and MNAR data conditions according to [@stekhoven2012missforest]. The application of K-Nearest Neighbors (KNN) and Random Forests along with Autoencoders in machine learning demonstrates exceptional ability to analyze high-dimensional data structures with non-linear relationships according to [@karim2024imputation] thoroughly examines state-of-the-art methods which show excellent performance in processing complex multi-dimensional data (2024).

4. Applications and Challenges:

With structured data sets suffering from missingness and such errors, method of imputation has a big effect on the quality of the analysis. While imputing traditionaly works fine for data with simple missingness patterns, a systematic review of 58 articles shows that in more complex casus only advanced technologies such as deep learning are better able to handle this issue out [@afkanpour2024identify]. But challenges remain, including the computational workload and lack of widely accepted methods for assessing results, which act as barriers to employing these new standards [@alwateer2024missing]. Discussing the potential and pitfalls of m method of imputation for datasets their epidemiological research is based on [@sterne2009multiple] particularly emphasize model specification and the need for sensictivity analysis to see if results are being biased in any one direction.

5. Semi-Parametric and Hybrid Approaches:

The hybrid approach is increasingly and becoming popular with researchers Just see the number of references to it on Google Scholar this year
Outperforming fully parametric approaches in social science research, semi-parametric methods such as predictive mean matching and hot deck imputation show great potential for item-nonresponse data [@durrant2005imputation].
Hybrid methods, combining traditional methods and machine learning, also deliver satisfactory results in increasing imputation accuracies without increasing computational time at all [@afkanpour2024identify].It is noted by [@lee2024prevention] that in clinical research settings, hybrid ones may well operate. Whether because a better preventive device with more sophisticated analytics yields even worse data quality than ever before; or due to some other double-combining of tools and methods, weightless chances must not be missed out on because of their particular novelty [@farhangfar2007novel]. 


*This is my work and I want to add more work...*

## Methods

-   Detail the models or algorithms used.

-   Justify your choices based on the problem and data.

*The common non-parametric regression model is*
$Y_i = m(X_i) + \varepsilon_i$*, where* $Y_i$ *can be defined as the sum
of the regression function value* $m(x)$ *for* $X_i$*. Here* $m(x)$ *is
unknown and* $\varepsilon_i$ *some errors. With the help of this
definition, we can create the estimation for local averaging i.e.*
$m(x)$ *can be estimated with the product of* $Y_i$ *average and* $X_i$
*is near to* $x$*. In other words, this means that we are discovering
the line through the data points with the help of surrounding data
points. The estimation formula is printed below [@R-base]:*

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$$W_n(x)$ *is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if* $X_i$ *is far from* $x$*.*

*Another equation:*

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data Exploration and Visualization

-   Describe your data sources and collection process.

-   Present initial findings and insights through visualizations.

-   Highlight unexpected patterns or anomalies.

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}

```

### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References
