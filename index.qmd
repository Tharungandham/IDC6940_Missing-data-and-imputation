---
title: "Missing Data and Imputation"
author: "Tharun Teja Gandham & Mourya Rai Papolu (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
header-includes:
  - |
    <style>
      body {
        background: linear-gradient(to right, #7db5e3, #d3d3d3); /* Change this color as needed */
      }
    </style>
---


## Introduction

Across healthcare, social sciences and finance sectors missing data continues as an extensive problem because it creates biased outcomes and reduces research reliability and weakens research conclusions unless appropriate data handling methods are used [@salgado2016missing]. The Electronic Health Records (EHR) dataset faces high vulnerability to missing data from technological breakdowns as well as human mistakes and data loss incidents. In various fields missing data originates from survey non-responses and equipment failures as well as data entry errors [@alwateer2024missing]. Improper handling of missing data leads to severe consequences on the quality of research findings and decision-making processes particularly when analyzing clinical epidemiology and structured datasets. 

The solution for this issue requires researchers to implement imputation techniques that provide estimated values for missing data points. Various imputation methods exist which include basic methods like mean substitution and advanced techniques using multiple imputation along with machine learning models. The selection of a method for imputation requires knowledge of the missing data pattern because it can be MCAR or MAR or MNAR. Mean imputation and listwise deletion remain straightforward to use yet they produce biases or overlook missing value uncertainty [@pedersen2017missing].Multiple imputation and K-Nearest Neighbors (KNN) and Random Forests machine learning models effectively manage complex datasets that have high missingness levels and non-linear relationships. Proper evaluation procedures must be conducted because these techniques need to balance both computational efficiency and accuracy levels.

The selection of imputation methods relies on both the dataset characteristics and the nature of missing data according to recent research findings. Research on 58 articles demonstrates traditional methods work best for data sets with straightforward missingness patterns but deep learning and hybrid models deliver superior results for complex data patterns. The social science field demonstrates that semi-parametric techniques including predictive mean matching with hot deck imputation deliver superior results compared to fully parametric methods for item-nonresponse data according to [@durrant2005imputation]. The research emphasizes the requirement for imputation approaches which match the unique needs of datasets along with their associated research goals.

The analysis evaluates five imputation approaches based on mean, mode, regression, K-Nearest Neighbors (KNN), and Multiple Imputation by Chained Equations (MICE) through application on the well-known Titanic dataset. The research objective is to determine which of these five methods produces the best results for handling missing data in structured datasets so researchers can guarantee data reliability and improve their research findings' validity. The research findings from this analysis will expand existing knowledge about imputation methods and supply actionable tips to scientists and practitioners who need to handle missing data.


## Literature Review

Missing data is common problem in data science, which affects the accuracy and reliability of analyses across different domains. This section summarizes major findings from existing research on ‘missing data imputation’, which focus on old traditional and new advanced methods and their applications, limitations.

1. Mechanisms of Missing Data:

How missing data is dealt with in practice depends entirely on what type of missingness it has, so that it can be classified as Missing Completely at Random (MCAR), Missing at Random (MAR), or Not Missing at Random (NMAR). In terms of clinical research, MAR is a very common means of missing data, often being the result of careening into trees or equipment breaking [@salgado2016missing]. Identifying these mechanisms is important for correctly chosen allocation mechanisms. [@lee2024prevention] stress that robustness across time for data acquisition must be used to prevent missing data in clinical studies together with monitoring as close to real-time as possible.

2. Traditional Imputation Methods:

Traditional methods such as mean substitution, regression imputation, and listwise deletion are simple, widely used, but also introduce bias or reduce statistical power. When mean substitution is performed, the original distribution of data is disrupted as one point is substituting every missing figure. It may be biased or not exceed thoroughness in cases where people can participate for complete information and which doesn't require objectives. Sample size is reduced by listwise deletion hence leading to loss of statistical power [@alwateer2024missing]. However, traditional methods still remain popular because of their simplicity of implementation and the manner in which they focus on bias rather than noise in data. [@yadav2024computational] offer a comprehensive overview of these methods, pointing out that their main employment is low-dimensional data sets with simple missing patterns

3. Advanced Imputation Techniques:

Multiple imputation alongside machine learning and deep learning models demonstrate effectiveness when dealing with complex datasets having substantial amounts of missing data. Referred to by Little & Rubin [@yadav2024computational] Multiple Imputation by Chained Equations (MICE) produces numerous datasets with imputed values which it then combines to deal with uncertainty in clinical research. The algorithm MissForest uses Random Forests to analyze mixed-type data while showing reliable results across MCAR, MAR, and MNAR data conditions according to [@stekhoven2012missforest]. The application of K-Nearest Neighbors (KNN) and Random Forests along with Autoencoders in machine learning demonstrates exceptional ability to analyze high-dimensional data structures with non-linear relationships according to [@karim2024imputation] thoroughly examines state-of-the-art methods which show excellent performance in processing complex multi-dimensional data (2024).

4. Applications and Challenges:

With structured data sets suffering from missingness and such errors, method of imputation has a big effect on the quality of the analysis. While imputing traditionaly works fine for data with simple missingness patterns, a systematic review of 58 articles shows that in more complex casus only advanced technologies such as deep learning are better able to handle this issue out [@afkanpour2024identify]. But challenges remain, including the computational workload and lack of widely accepted methods for assessing results, which act as barriers to employing these new standards [@alwateer2024missing]. Discussing the potential and pitfalls of m method of imputation for datasets their epidemiological research is based on [@sterne2009multiple] particularly emphasize model specification and the need for sensictivity analysis to see if results are being biased in any one direction.

5. Semi-Parametric and Hybrid Approaches:

The hybrid approach is increasingly and becoming popular with researchers Just see the number of references to it on Google Scholar this year
Outperforming fully parametric approaches in social science research, semi-parametric methods such as predictive mean matching and hot deck imputation show great potential for item-nonresponse data [@durrant2005imputation].
Hybrid methods, combining traditional methods and machine learning, also deliver satisfactory results in increasing imputation accuracies without increasing computational time at all [@afkanpour2024identify].It is noted by [@lee2024prevention] that in clinical research settings, hybrid ones may well operate. Whether because a better preventive device with more sophisticated analytics yields even worse data quality than ever before; or due to some other double-combining of tools and methods, weightless chances must not be missed out on because of their particular novelty [@farhangfar2007novel]. 


*This is my work and I want to add more work...*

## Imputation Methods

-   Detail the models or algorithms used.

#### 1.Mean and Mode Imputation

Mean and mode imputation are simple methods that replace missing values with the average (for numerical data) or most common value (for categorical data) of the observed values.

Imputation with mean involves substituting missing values with the arithmetic mean calculated from observed values.

This method replaces missing values by using the mode which represents the most frequent value present in observed data points.

The implementation of these methods remains straightforward yet data distortion may occur when missing values exceed a certain threshold [@pedersen2017missing] [@afkanpour2024identify].

The mean imputation formula is:

$$
\hat{x}_i = \bar{x} = \frac{1}{n} \sum_{j=1}^{n} x_j
$$
	
where x^i is the imputed value, xˉ is the mean of the observed values, n is the number of observed values, and xj is the j-th observed value [@pedersen2017missing] [@afkanpour2024identify].

 Mode Imputation:
$$
\hat{x}_i=mode(x)
$$
where mode(x)mode(x) is the most frequently occurring value in the dataset [@pedersen2017missing] [@afkanpour2024identify].

	

#### 2.Regression Imputation
A regression model enables regression imputation to predict missing values by analyzing the connections between other variables in the dataset. For example:

The Age column's missing values can be predicted with the help of Pclass and Fare variables.

The method requires linear variable relationships as its foundation but provides advanced handling compared to mean/mode imputation (Little & Rubin, 2002; [@farhangfar2007novel].

Regression Model:
The regression imputation formula is:

$$
\begin{align}
\hat{x}_i &= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \epsilon_i \\
\text{where} \quad \beta_0 &= \text{intercept}, \quad \beta_1, \dots, \beta_p = \text{coefficients}
\end{align}
$$
where x^i is the predicted value, β0,β1,…,βp are the regression coefficients, and ϵi is the error term [@yadav2024computational]; [@little2019statistical].

#### 3.K-Nearest Neighbors (KNN) Imputation
   KNN imputation allows advanced replacement of missing values through an analysis of similar data points (neighbors) in the dataset. Here’s how it works:

The KNN algorithm locates k data points with similar features as each missing value to make its imputation decision.

The algorithm obtains the average measurement point for numerical data and most frequent value for categorical data from the k nearest neighbors.

The impute.knn() function from the impute package in R enables users to execute KNN imputation. KNN proves useful when data points that are similar in nature tend to possess similar values [@alwateer2024missing]; .

KNN Formula:
The KNN imputation formula is:

$$
\hat{x}_i = \frac{1}{k} \sum_{j \in N_k(i)} x_j \label{eq:knn}
$$

As shown in Equation , KNN imputation replaces missing values with the average of the nearest neighbors.
where x^i is the imputed value, Nk(i) is the set of k nearest neighbors, and xj is the value of the j-th neighbor [@alwateer2024missing] [@stekhoven2012missforest].

#### 4.Multiple Imputation by Chained Equations (MICE)

MICE is a powerful method that handles missing data by creating multiple complete versions of the dataset. Here’s how it works:

It starts by filling in the missing values with simple guesses (e.g., the mean or median).

Then, it uses regression models to predict the missing values based on other variables in the dataset.

This process is repeated several times, and the results are combined to produce a final, more accurate dataset.

In R, you can use the mice() function from the mice package to perform MICE imputation. This method is especially useful for datasets with complex patterns of missing data [@little2019statistical] White et al., 2011).



-   Justify your choices based on the problem and data.




## Analysis and Results

### Data Exploration and Visualization

-   Describe your data sources and collection process.

    (https://www.kaggle.com/c/titanic/data)
    
  The Titanic dataset from Kaggle is a widely used dataset for data analysis and machine learning projects. It contains information about the passengers aboard the Titanic, including whether they survived or not. The dataset includes features such as:
      
- PassengerId: A unique identifier for each passenger.
-	Survived: Whether the passenger survived (1) or not (0).
-	Pclass: The ticket class (1st, 2nd, or 3rd), which is a proxy for socio-economic status.
-	Name: The name of the passenger.
-	Sex: The gender of the passenger (male or female).
-	Age: The age of the passenger.
-	SibSp: The number of siblings/spouses aboard the Titanic.
-	Parch: The number of parents/children aboard the Titanic.
-	Ticket: The ticket number.
-	Fare: The fare paid for the ticket.
-	Cabin: The cabin number (many missing values).
-	Embarked: The port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton).


```{r}
# Set CRAN mirror
options(repos = c(CRAN = "https://cloud.r-project.org"))

# Install and load ggplot2
if (!require(ggplot2)) install.packages("ggplot2")
library(ggplot2)

# Install and load naniar
if (!require(naniar)) install.packages("naniar")
library(naniar)


```


```{r}
library(dplyr)
#titanic dataset
train<-read.csv("train.csv")
# Replace empty strings with NA in specific columns
train <- train %>%
  mutate(
    Cabin = na_if(Cabin, ""),
    Embarked = na_if(Embarked, "")
  )
summary(train)
```
```{r}
#missing values column wise
colSums(is.na(train))
```

- Age column has 177 missing values 
- Cabin column has large number of missing values 687
- Similarly embarked column has 2 missing values.

##### Unexpected patterns or anomalies.

```{r}
# creating a data frame to show the missing values percentage
missing_data<-data.frame(
  column_name=names(train),
  missing_val=colSums(is.na(train)),
  missing_percent=colSums(is.na(train)/nrow(train))*100
)

print(missing_data)

```
The Titanic dataset has missing values primarily in the Age and Cabin columns, with 177 (19.87%) and 687 (77.10%) missing entries, respectively. The Embarked column has only 2 (0.22%) missing values, which is negligible. All other columns, such as PassengerId, Survived, and Sex, are complete with no missing data, making them reliable for analysis.


```{r}

#visualization of my data set through ggplot
ggplot(missing_data,aes(x=column_name,y=missing_val))+
  geom_bar(stat="identity",fill="blue")+
  labs(title="missing values in titanic data set", x="column_names", y="number of missing values")+
  theme_bw()

```

```{r}

vis_miss(train)+
  labs(title="missingness pattern in dataset")
```

```{r}
#Whether the passenger survived (1) or not (0)
ggplot(train, aes(x = factor(Survived))) +  
  geom_bar(fill = "steelblue") +  
  labs(title = "Survival Count", x = "Survived", y = "Count")  
```

The plot shows that fewer passengers survived than crew members. This depiction emphasizes the survival rate imbalance in the data since it strongly affects both analytical processes and model predictions of survival estimates.

```{r}
# Age Distribution  
ggplot(train, aes(x = Age)) +  
  geom_histogram(binwidth = 5, fill = "gray", color = "black") +  
  labs(title = "Age Distribution", x = "Age", y = "Count")  

```

The density plot shows most passengers fell between 20 and 40 while showing their highest count at age 28 during the voyage. The passenger population shows scarcity among those younger than ten years old and older than sixty years old. The plotted distribution allows researchers to understand population statistics about the ship passengers which helps explain survival patterns based on passenger ages.



### Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}

```

### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References
