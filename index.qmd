---
title: "Missing Data and Imputation"
author: "Tharun Teja Gandham & Mourya Rai Papolu (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
header-includes:
  - |
    <style>
      body {
        background: linear-gradient(to right, #7db5e3, #d3d3d3); /* Change this color as needed */
      }
    </style>
---
Slides: [slides.html](slides.html){target="_blank"} ()

## Introduction

Across healthcare, social sciences and finance sectors missing data continues as an extensive problem because it creates biased outcomes and reduces research reliability and weakens research conclusions unless appropriate data handling methods are used [@salgado2016missing]. The Electronic Health Records (EHR) dataset faces high vulnerability to missing data from technological breakdowns as well as human mistakes and data loss incidents. In various fields missing data originates from survey non-responses and equipment failures as well as data entry errors [@alwateer2024missing]. Improper handling of missing data leads to severe consequences on the quality of research findings and decision-making processes particularly when analyzing clinical epidemiology and structured datasets. 

The solution for this issue requires researchers to implement imputation techniques that provide estimated values for missing data points. Various imputation methods exist which include basic methods like mean substitution and advanced techniques using multiple imputation along with machine learning models. The selection of a method for imputation requires knowledge of the missing data pattern because it can be MCAR or MAR or MNAR. Mean imputation and listwise deletion remain straightforward to use yet they produce biases or overlook missing value uncertainty [@pedersen2017missing].Multiple imputation and K-Nearest Neighbors (KNN) and Random Forests machine learning models effectively manage complex datasets that have high missingness levels and non-linear relationships. Proper evaluation procedures must be conducted because these techniques need to balance both computational efficiency and accuracy levels.

The selection of imputation methods relies on both the dataset characteristics and the nature of missing data according to recent research findings. Research on 58 articles demonstrates traditional methods work best for data sets with straightforward missingness patterns but deep learning and hybrid models deliver superior results for complex data patterns. The social science field demonstrates that semi-parametric techniques including predictive mean matching with hot deck imputation deliver superior results compared to fully parametric methods for item-nonresponse data according to [@durrant2005imputation]. The research emphasizes the requirement for imputation approaches which match the unique needs of datasets along with their associated research goals.

The analysis evaluates five imputation approaches based on mean, mode, regression, K-Nearest Neighbors (KNN), and Multiple Imputation by Chained Equations (MICE) through application on the well-known Titanic dataset. The research objective is to determine which of these five methods produces the best results for handling missing data in structured datasets so researchers can guarantee data reliability and improve their research findings' validity. The research findings from this analysis will expand existing knowledge about imputation methods and supply actionable tips to scientists and practitioners who need to handle missing data.


### Literature Review

Missing data is common problem in data science, which affects the accuracy and reliability of analyses across different domains. This section summarizes major findings from existing research on ‘missing data imputation’, which focus on old traditional and new advanced methods and their applications, limitations.

1. Mechanisms of Missing Data:

How missing data is dealt with in practice depends entirely on what type of missingness it has, so that it can be classified as Missing Completely at Random (MCAR), Missing at Random (MAR), or Not Missing at Random (NMAR). In terms of clinical research, MAR is a very common means of missing data, often being the result of careening into trees or equipment breaking [@salgado2016missing]. Identifying these mechanisms is important for correctly chosen allocation mechanisms. [@lee2024prevention] stress that robustness across time for data acquisition must be used to prevent missing data in clinical studies together with monitoring as close to real-time as possible.

2. Traditional Imputation Methods:

Traditional methods such as mean substitution, regression imputation, and listwise deletion are simple, widely used, but also introduce bias or reduce statistical power. When mean substitution is performed, the original distribution of data is disrupted as one point is substituting every missing figure. It may be biased or not exceed thoroughness in cases where people can participate for complete information and which doesn't require objectives. Sample size is reduced by listwise deletion hence leading to loss of statistical power [@alwateer2024missing]. However, traditional methods still remain popular because of their simplicity of implementation and the manner in which they focus on bias rather than noise in data. [@yadav2024computational] offer a comprehensive overview of these methods, pointing out that their main employment is low-dimensional data sets with simple missing patterns

3. Advanced Imputation Techniques:

Multiple imputation alongside machine learning and deep learning models demonstrate effectiveness when dealing with complex datasets having substantial amounts of missing data. Referred to by Little & Rubin [@yadav2024computational] Multiple Imputation by Chained Equations (MICE) produces numerous datasets with imputed values which it then combines to deal with uncertainty in clinical research. The algorithm MissForest uses Random Forests to analyze mixed-type data while showing reliable results across MCAR, MAR, and MNAR data conditions according to [@stekhoven2012missforest]. The application of K-Nearest Neighbors (KNN) and Random Forests along with Autoencoders in machine learning demonstrates exceptional ability to analyze high-dimensional data structures with non-linear relationships according to [@karim2024imputation] thoroughly examines state-of-the-art methods which show excellent performance in processing complex multi-dimensional data (2024).

4. Applications and Challenges:

With structured data sets suffering from missingness and such errors, method of imputation has a big effect on the quality of the analysis. While imputing traditionaly works fine for data with simple missingness patterns, a systematic review of 58 articles shows that in more complex casus only advanced technologies such as deep learning are better able to handle this issue out [@afkanpour2024identify]. But challenges remain, including the computational workload and lack of widely accepted methods for assessing results, which act as barriers to employing these new standards [@alwateer2024missing]. Discussing the potential and pitfalls of m method of imputation for datasets their epidemiological research is based on [@sterne2009multiple] particularly emphasize model specification and the need for sensictivity analysis to see if results are being biased in any one direction.

5. Semi-Parametric and Hybrid Approaches:

The hybrid approach is increasingly and becoming popular with researchers Just see the number of references to it on Google Scholar this year
Outperforming fully parametric approaches in social science research, semi-parametric methods such as predictive mean matching and hot deck imputation show great potential for item-nonresponse data [@durrant2005imputation].
Hybrid methods, combining traditional methods and machine learning, also deliver satisfactory results in increasing imputation accuracies without increasing computational time at all [@afkanpour2024identify].It is noted by [@lee2024prevention] that in clinical research settings, hybrid ones may well operate. Whether because a better preventive device with more sophisticated analytics yields even worse data quality than ever before; or due to some other double-combining of tools and methods, weightless chances must not be missed out on because of their particular novelty [@farhangfar2007novel]. 


## Methods

#### 1.Mean and Mode Imputation

Mean and mode imputation are simple methods that replace missing values with the average (for numerical data) or most common value (for categorical data) of the observed values.

Imputation with mean involves substituting missing values with the arithmetic mean calculated from observed values.

This method replaces missing values by using the mode which represents the most frequent value present in observed data points.

The implementation of these methods remains straightforward yet data distortion may occur when missing values exceed a certain threshold [@pedersen2017missing] [@afkanpour2024identify].

The mean imputation formula is:

$$
\hat{x}_i = \bar{x} = \frac{1}{n} \sum_{j=1}^{n} x_j
$$
	
where x^i is the imputed value, xˉ is the mean of the observed values, n is the number of observed values, and xj is the j-th observed value [@pedersen2017missing] [@afkanpour2024identify].

 Mode Imputation:
$$
\hat{x}_i=mode(x)
$$
where mode(x)mode(x) is the most frequently occurring value in the dataset [@pedersen2017missing] [@afkanpour2024identify].

	

#### 2.Regression Imputation
A regression model enables regression imputation to predict missing values by analyzing the connections between other variables in the dataset. For example:

The Age column's missing values can be predicted with the help of Pclass and Fare variables.

The method requires linear variable relationships as its foundation but provides advanced handling compared to mean/mode imputation (Little & Rubin, 2002; [@farhangfar2007novel].

Regression Model:
The regression imputation formula is:

$$
\begin{align}
\hat{x}_i &= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \epsilon_i \\
\text{where} \quad \beta_0 &= \text{intercept}, \quad \beta_1, \dots, \beta_p = \text{coefficients}
\end{align}
$$
where x^i is the predicted value, β0,β1,…,βp are the regression coefficients, and ϵi is the error term [@yadav2024computational]; [@little2019statistical].

#### 3.K-Nearest Neighbors (KNN) Imputation
   K-Nearest Neighbors (KNN) is a machine learning algorithm used for both classification and regression tasks. In the context of missing data imputation:

KNN finds the K most similar rows (neighbors) in the dataset based on the values of other features.

It then uses the values from these neighbors to estimate the missing values.
For each row with a missing value, KNN looks at the other features (columns) in the dataset.It calculates the distance (e.g., Euclidean distance) between the row with the missing value and all other rows.

It selects the K nearest neighbors (rows) that are most similar based on the distance.

The algorithm obtains the average measurement point for numerical data and most frequent value for categorical data from the k nearest neighbors.

The impute.knn() function from the impute package in R enables users to execute KNN imputation. KNN proves useful when data points that are similar in nature tend to possess similar values [@alwateer2024missing]; .

KNN Formula:
The KNN imputation formula is:

$$
\hat{x}_i = \frac{1}{k} \sum_{j \in N_k(i)} x_j \label{eq:knn} 
$$
k  Specifies the number of nearest neighbors to use.

As shown in Equation , KNN imputation replaces missing values with the average of the nearest neighbors.
where x^i is the imputed value, Nk(i) is the set of k nearest neighbors, and xj is the value of the j-th neighbor [@alwateer2024missing] [@stekhoven2012missforest].

#### 4.Multiple Imputation by Chained Equations (MICE)
MICE serves as a popular method for missing data management through its creation of numerous complete versions of original datasets. MICE proves effective in dealing with complex missing data patterns provided data follow the MAR assumption. The following description of MICE features a detailed analysis supported by references from Pedersen et al. (2017). 

How MICE Works

- Initialization:

MICE starts by filling in the missing values with initial guesses (e.g., mean, median, or random values). These are temporary placeholders and will be refined in subsequent steps (Pedersen et al., 2017).

- Iterative Imputation:

Each variable with missing data within MICE undergoes regression model analysis to generate predicted values by using other dataset variables. A model selection depends on the variable type for regression analysis.
Linear regression for numeric variables.
Logistic regression for binary categorical variables.
Multinomial regression for multi-class categorical variables (Pedersen et al., 2017).
The imputation process runs repeatedly while updating missing value predictions during each cycle until prediction changes reach an unnoticeable point.

- Multiple Imputed Datasets:

The MICE method creates five to ten datasets which represent diverse plausible alternatives for missing data values. The method represents the unknown nature of missing data values (Pedersen et al., 2017).

- Combining Results:

Each data set created through MICE receives separate analysis for the study interest such as logistic regression. Combines the results from these datasets to provide unbiased and valid estimates of associations according to Pedersen et al. (2017).

- For now we are proceeding with mean/mode , regression and knn imputations.

## Data Exploration and Visualization

-   Describe your data sources and collection process.

    (https://www.kaggle.com/c/titanic/data)
    
  The Titanic dataset from Kaggle is a widely used dataset for data analysis and machine learning projects. It contains information about the passengers aboard the Titanic, including whether they survived or not. The dataset includes features such as:
      
- PassengerId: A unique identifier for each passenger.
-	Survived: Whether the passenger survived (1) or not (0).
-	Pclass: The ticket class (1st, 2nd, or 3rd), which is a proxy for socio-economic status.
-	Name: The name of the passenger.
-	Sex: The gender of the passenger (male or female).
-	Age: The age of the passenger.
-	SibSp: The number of siblings/spouses aboard the Titanic.
-	Parch: The number of parents/children aboard the Titanic.
-	Ticket: The ticket number.
-	Fare: The fare paid for the ticket.
-	Cabin: The cabin number (many missing values).
-	Embarked: The port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton).


```{r}
# Set CRAN mirror
options(repos = c(CRAN = "https://cloud.r-project.org"))

# Install and load ggplot2
if (!require(ggplot2)) install.packages("ggplot2")
library(ggplot2)

# Install and load naniar
if (!require(naniar)) install.packages("naniar")
library(naniar)


```


```{r}
library(dplyr)
#titanic dataset
train<-read.csv("train.csv")
# Replace empty strings with NA in specific columns
train <- train %>%
  mutate(
    Cabin = na_if(Cabin, ""),
    Embarked = na_if(Embarked, "")
  )
summary(train)
```
```{r}
#missing values column wise
colSums(is.na(train))
```

- Age column has 177 missing values 
- Cabin column has large number of missing values 687
- Similarly embarked column has 2 missing values.

##### Unexpected patterns or anomalies.

```{r}
# creating a data frame to show the missing values percentage
missing_data<-data.frame(
  column_name=names(train),
  missing_val=colSums(is.na(train)),
  missing_percent=colSums(is.na(train)/nrow(train))*100
)

print(missing_data)

```
The Titanic dataset has missing values primarily in the Age and Cabin columns, with 177 (19.87%) and 687 (77.10%) missing entries, respectively. The Embarked column has only 2 (0.22%) missing values, which is negligible. All other columns, such as PassengerId, Survived, and Sex, are complete with no missing data, making them reliable for analysis.


```{r}

#visualization of my data set through ggplot
ggplot(missing_data,aes(x=column_name,y=missing_val))+
  geom_bar(stat="identity",fill="blue")+
  labs(title="missing values in titanic data set", x="column_names", y="number of missing values")+
  theme_bw()

```
- each column and their missingness compared in bar chart, cabin column has highest number of missing values.

```{r}

vis_miss(train)+
  labs(title="missingness pattern in dataset")
```


```{r}
#Whether the passenger survived (1) or not (0)
ggplot(train, aes(x = factor(Survived))) +  
  geom_bar(fill = "steelblue") +  
  labs(title = "Survival Count", x = "Survived", y = "Count")  
```


The plot shows that fewer passengers survived . This depiction emphasizes the survival rate imbalance in the data since it strongly affects both analytical processes and model predictions of survival estimates.

```{r}
ggplot(train, aes(x = Sex, fill = factor(Survived))) +
  geom_bar(position = "fill") +
  labs(title = "Survival Rate by Gender", x = "Gender", y = "Proportion", fill = "Survived")
```

- we can see that more female passengers are survived than male passengers from total number of passengers.

```{r}
ggplot(train, aes(x = factor(Pclass), fill = factor(Survived))) +
  geom_bar(position = "fill") +
  labs(title = "Survival Rate by Passenger Class", x = "Class", y = "Proportion", fill = "Survived")
```

The stacked bar chart illustrates the survival rate of passengers by class on the Titanic. First-class passengers had the highest proportion of survivors, followed by second class, while third-class passengers had the lowest survival rate. Notably, the majority of third-class passengers did not survive, highlighting the disparity in survival chances based on socioeconomic status. This suggests that higher-class passengers likely had better access to lifeboats or evacuation assistance. The visualization underscores how passenger class played a significant role in determining survival during the disaster.


```{r}
ggplot(train, aes(x = factor(Survived), y = Age, fill = factor(Survived))) +
  geom_boxplot() +
  labs(title = "Age Distribution by Survival", x = "Survived", y = "Age", fill = "Survived")
```

- We can see that most survivors are in the 20–40 age range, so missing ages for survivors might be imputed with a value in that range.
- Median Age: The median age of survivors (blue box) is slightly higher than that of non-survivors (red box).
- Age Spread: Both groups have a similar spread of ages, meaning age alone may not be a strong factor in survival.
-Most survivors (blue) and non-survivors (red) fall within 20–40 years, with survivors having a marginally higher median age. The overlapping IQRs indicate age’s limited predictive power unless combined with other features (e.g., class or gender)."

```{r}
# Age Distribution  
ggplot(train, aes(x = Age)) +  
  geom_histogram(binwidth = 5, fill = "gray", color = "black") +  
  labs(title = "Age Distribution", x = "Age", y = "Count")  

```

The histogram displays the distribution of passenger ages. The majority of passengers fall within the 20–40 age range, with a peak around the early twenties. A smaller number of passengers are younger children or older adults, with very few above 60. The distribution is right-skewed, indicating more younger passengers compared to older ones. This visualization helps understand the overall age demographics and can guide imputation strategies for missing age values.


```{r}
library(reshape2)

# Select numeric columns
numeric_data <- train %>%
  select(Age, Fare, SibSp, Parch, Survived)

# Compute correlation matrix
cor_matrix <- cor(numeric_data, use = "complete.obs")

# Melt the correlation matrix for ggplot
melted_cor_matrix <- melt(cor_matrix)

# Plot heatmap
ggplot(melted_cor_matrix, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0) +
  labs(title = "Correlation Heatmap", x = "", y = "", fill = "Correlation") +
  theme_bw()
```

This correlation heatmap provides insights into relationships between key numerical variables in our dataset. Darker red indicates a strong positive correlation, while purple represents a negative correlation. For example, 'Fare' and 'Survived' show a positive correlation, suggesting passengers who paid higher fares had a higher survival rate. Identifying these relationships helps in understanding data patterns, which is crucial when handling missing values, as correlations can guide imputation strategies.

The Pearson correlation coefficient ($r$) measures the linear relationship between two variables, ranging from $-1$ to $+1$. It is computed as:  
$$
\begin{align}
r &= \frac{n(\sum XY) - (\sum X)(\sum Y)}{\sqrt{[n \sum X^2 - (\sum X)^2][n \sum Y^2 - (\sum Y)^2]}} \\
\text{where} \quad n &= \text{number of observations}, \\
X, Y &= \text{paired variables}, \\
\sum XY &= \text{sum of cross-products}, \\
\sum X, \sum Y &= \text{sums of individual variables}.
\end{align}
$$


## Analysis and Results

#### Mean and Mode Imputation

```{r}
# copy of the original data set
train_imputed1<- train
```

```{r}
# Mean imputation for Age( numeric column)
mean_age <- mean(train_imputed1$Age, na.rm = TRUE) #(remove na while caluclating mean)
train_imputed1 <- train_imputed1 %>%
  mutate(Age = ifelse(is.na(Age), mean_age, Age))

```

```{r}
# Mode imputation replaces missing values in categorical columns (e.g., Embarked) with the mode (most frequent value) of the column.

# Mode imputation for Embarked
mode_embarked <- names(sort(table(train_imputed1$Embarked), decreasing = TRUE))[1]
train_imputed1 <- train_imputed1 %>%
  mutate(Embarked = ifelse(is.na(Embarked), mode_embarked, Embarked))

# Mode imputation for Cabin
mode_cabin <- names(sort(table(train_imputed1$Cabin), decreasing = TRUE))[1]
train_imputed1 <- train_imputed1 %>%
  mutate(Cabin = ifelse(is.na(Cabin), mode_cabin, Cabin))


colSums(is.na(train_imputed1))
```
```{r}
#  Visualizing Age distribution after imputation
ggplot(train_imputed1, aes(x = Age)) +
  geom_histogram(binwidth = 5, fill = "gray", color = "black") +
  labs(title = "Age Distribution After Mean Imputation", x = "Age", y = "Count")
```
The histogram now shows a sharp peak around age 28–30, with counts nearing 300, due to mean/mode imputation. The previously dominant 20–40 age range has reduced counts (100–150), as missing values were replaced by the central tendency. This creates an artificial cluster, reducing natural age variability. The change highlights how imputation can distort distributions, potentially affecting analysis.


```{r}
ggplot(train_imputed1, aes(x = factor(Survived), y = Age, fill = factor(Survived))) +
  geom_boxplot() +
  labs(title = "Age Distribution by Survival", x = "Survived", y = "Age", fill = "Survived")
```

- After imputation, ages cluster sharply around 28–30, losing the original spread (20–40 years).

- This artificial spike may distort survival predictions, overemphasizing the imputed age.

- Results could be biased.

##### REGRESSION IMPUTATION
```{r}
#linear regression for age (numerical data)
train_regression<-train
# Fit the linear regression model
age_model <- lm(Age ~ Pclass + Sex + SibSp + Parch + Fare, data = train_regression)

# Identify rows with missing Age
missing_age <- is.na(train_regression$Age)

# Predict missing Age values
train_regression$Age[missing_age] <- predict(age_model, newdata = train_regression[missing_age, ])

```

```{r}

sum(is.na(train_regression$Embarked))
```

Since Embarked column has 3 categories: "C", "Q", "S" we are using Multinomial Logistic Regression for Embarked.
```{r}
library(nnet)

# Fit a multinomial logistic regression model for Embarked
embarked_model <- multinom(Embarked ~ Pclass + Sex + Age + SibSp + Parch + Fare, 
                           data = train_regression)
missing_embarked <- is.na(train_regression$Embarked)

predicted_embarked <- predict(embarked_model, newdata = train_regression[missing_embarked, ], type = "class")

# Replace missing values with predicted values
train_regression$Embarked[missing_embarked] <- predicted_embarked

```

- Too Many Missing Values: 687 missing values out of 891 is a lot, and regression imputation might not work well due to the lack of sufficient data.
- The Cabin column has many unique categories (e.g., "G6", "C103", "C123", etc.), making regression imputation computationally expensive and less reliable.
- So i am using mode imputation for this column.
```{r}
# Mode imputation for Cabin
mode_reg <- names(sort(table(train_regression$Cabin), decreasing = TRUE))[1]
train_regression <- train_regression %>%
  mutate(Cabin = ifelse(is.na(Cabin), mode_reg, Cabin))


colSums(is.na(train_regression))
```
using regression imputation techiniques i Replaced NA values .

##### KNN IMPUTATION

```{r}

# Load the VIM package
library(VIM)

# Perform KNN imputation
train_knn <- kNN(train, k = 5)


# Check missing values after imputation
colSums(is.na(train_knn))
```


### Modeling and Results
#### Comparing Age distributions

```{r}

library(ggplot2)


ggplot(train, aes(Age)) + 
  geom_histogram(fill="red", alpha=0.5, bins=30) + 
  ggtitle("Original Age Distribution") +
  theme_minimal()

ggplot(train_imputed1, aes(Age)) + 
  geom_histogram(fill="blue", alpha=0.5, bins=30) + 
  ggtitle("Age Distribution after Mean Imputation") +
  theme_minimal()

ggplot(train_regression, aes(Age)) + 
  geom_histogram(fill="green", alpha=0.5, bins=30) + 
  ggtitle("Age Distribution after Regression Imputation") +
  theme_minimal()

ggplot(train_knn,aes(Age)) +
  geom_histogram(fill="gold",alpha=0.5, bins=30)+
  ggtitle("Age Distribution after knn Imputation") +
  theme_minimal()
```

- Original Data shows a natural spread of passenger ages (mostly 20-40 years).

- Mean Imputation creates an unrealistic spike at one age (28-30), making the data look artificial.

- Regression and KNN Imputation preserve the original pattern better - ages remain spread out like real data.

What This Means:
Mean imputation distorts the data by forcing all missing ages to be similar, while regression and KNN methods keep the natural variation we see in the original dataset. For accurate analysis, regression or KNN work better than simple mean replacement.

##### Model Performance
```{r}

model_1 <- Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked

# Train models on different imputed datasets
logit_mean <- glm(model_1, data = train_imputed1, family = "binomial")
logit_reg <- glm(model_1, data = train_regression, family = "binomial")
logit_knn <- glm(model_1, data = train_knn, family = "binomial")

```

```{r}
install.packages("caret", dependencies = TRUE)
library(caret)

```
```{r}
# Convert Survived column to factor (Required for confusionMatrix)
train_imputed1$Survived <- as.factor(train_imputed1$Survived)
train_regression$Survived <- as.factor(train_regression$Survived)
train_knn$Survived <- as.factor(train_knn$Survived)

# Get predictions (Convert probabilities to binary predictions: 1 if >0.5 else 0)
pred_mean <- ifelse(predict(logit_mean, type = "response") > 0.5, 1, 0)
pred_reg <- ifelse(predict(logit_reg, type = "response") > 0.5, 1, 0)
pred_knn <- ifelse(predict(logit_knn, type = "response") > 0.5, 1, 0)

# Convert predictions and actual values to factors for confusionMatrix()
pred_mean <- as.factor(pred_mean)
pred_reg <- as.factor(pred_reg)
pred_knn <- as.factor(pred_knn)

# Compute confusion matrices
conf_matrix_mean <- confusionMatrix(pred_mean, train_imputed1$Survived)
conf_matrix_reg <- confusionMatrix(pred_reg, train_regression$Survived)
conf_matrix_knn <- confusionMatrix(pred_knn, train_knn$Survived)

# Print accuracy for each model
cat("Accuracy with Mean/Mode Imputation:", conf_matrix_mean$overall['Accuracy'], "\n")
cat("Accuracy with Regression Imputation:", conf_matrix_reg$overall['Accuracy'], "\n")
cat("Accuracy with KNN Imputation:", conf_matrix_knn$overall['Accuracy'], "\n")

```

- Results show that KNN imputation performed the best in terms of accuracy, followed by regression imputation, and then mean/mode imputation.

##### Model metrics

```{r}
metrics <- function(conf_matrix) {
  data.frame(
    Accuracy = conf_matrix$overall['Accuracy'],
    Sensitivity = conf_matrix$byClass['Sensitivity'],
    Specificity = conf_matrix$byClass['Specificity'],
    F1_Score = conf_matrix$byClass['F1']
  )
}

# Create comparison table
rbind(
  Mean = metrics(conf_matrix_mean),
  Regression = metrics(conf_matrix_reg),
  KNN = metrics(conf_matrix_knn)
)
```


```{r}
library(ggplot2)
library(tidyr)

# Prepare data
results <- data.frame(
  Method = c("Mean/Mode", "Regression", "KNN"),
  Accuracy = c(0.801, 0.808, 0.810),
  F1_Score = c(conf_matrix_mean$byClass['F1'], 
               conf_matrix_reg$byClass['F1'],
               conf_matrix_knn$byClass['F1'])
) %>% pivot_longer(-Method, names_to = "Metric")

# Plot
ggplot(results, aes(Method, value, fill = Metric)) +
  geom_col(position = "dodge") +
  labs(title = "Model Performance Comparison", 
       y = "Score", x = "Imputation Method") +
  scale_fill_brewer(palette = "Set1") +
  theme_minimal()
```

This comparison shows how different methods for handling missing data affect our model's predictions:

KNN imputation works best (0.81 accuracy), preserving complex patterns in the data.

Regression comes close (0.808 accuracy), using statistical relationships to fill gaps.

Simple mean/mode replacement performs worst (0.801 accuracy), as averages can't capture real-world variability.

The F1-scores (0.85 for KNN) confirm this ranking, especially important since our survival data is imbalanced."


```{r}
library(dplyr)  
library(broom)  
library(ggplot2)

# Get coefficients from each model
coef_mean <- tidy(logit_mean) %>% mutate(Method = "Mean/Mode")
coef_reg <- tidy(logit_reg) %>% mutate(Method = "Regression") 
coef_knn <- tidy(logit_knn) %>% mutate(Method = "KNN")

# Combine all coefficients
coefs <- rbind(coef_mean, coef_reg, coef_knn)

# Plot (simple version)
ggplot(coefs %>% filter(term != "(Intercept)"), 
       aes(x = term, y = estimate, color = Method)) +
  geom_point(size = 3) +
  coord_flip() +
  labs(title = "Columns Importance Across Models",
       x = "columns",
       y = "Coefficient Value") +
  theme_minimal()
```

- Gender Dominance: "Sexmale" shows the strongest negative impact (-8 to -10 coefficient), confirming males had significantly lower survival rates regardless of imputation method.

- Class & Wealth Matter: Pclass (negative) and Fare (positive) reveal poorer passengers (3rd class) fared worse, while wealthier ones (higher fares) survived more—consistent across all methods.

- Age’s  Role: Age hovers near zero, implying it had limited influence compared to gender/class, though KNN slightly increased its importance (closer to +1).

- Port Irrelevance: Embarked features (C/Q/S) show near-zero coefficients, proving boarding location had minimal survival impact—a useful insight for model simplification.

This confirms gender and wealth were primary survival drivers, while age/boarding port were secondary—critical for interpreting Titanic survival patterns accurately.

### Conclusion

Objective Achieved: Successfully compared three imputation methods (Mean/Mode, Regression, KNN) for handling missing data in the Titanic dataset.
Based on our imputation results, the missingness in the Titanic dataset is most likely Missing at Random (MAR).
This conclusion is supported by the higher performance of KNN imputation (accuracy: 0.81, F1-score: 0.85) and regression imputation (accuracy: 0.808), compared to simple mean/mode imputation (accuracy: 0.801).
These results suggest that the missing values are related to other observed variables, not completely random (MCAR) or dependent on unobserved data (MNAR).

##### Best Imputation Method:

KNN Imputation achieved the highest accuracy (81.0%) by intelligently filling missing values based on similar passengers.

Regression Imputation was a close second (80.8%), using statistical relationships.

Mean/Mode performed worst (80.1%), as it oversimplified the data.

##### Top Survival Influencers:

Gender (Sex_male) was the strongest predictor—being male drastically reduced survival chances.

Wealth (Fare and Pclass):

Higher fares improved survival.

Lower-class passengers (3rd class) had significantly worse outcomes.

Age had minor impact, while boarding port (Embarked) mattered very little.

#### Implications
For Data Quality:

Use KNN/Regression for accurate models—they preserve real patterns. Avoid mean imputation for critical tasks.

For Predictive Modeling:

Focus on gender, class, and fare—they drive survival predictions. Ignore weak factors (e.g., Embarked) to simplify models.

For Fairness:

Mean imputation’s distortion of age/class relationships could hide socioeconomic biases. Advanced methods (KNN) mitigate this.

#### Limitations
Computational Cost: KNN is slower than regression/mean—may not suit huge datasets.

Assumptions: Regression assumes linear relationships; KNN assumes similar neighbors exist.

Generalizability: Results are specific to the Titanic dataset. Test on other datasets to confirm trends.

#### Final Recommendation
"For reliable predictions, use KNN imputation and prioritize gender/class/wealth features. Mean imputation is only for quick drafts. Always check how missing data handling affects your model’s fairness and accuracy."



## References
